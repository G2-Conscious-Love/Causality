# Removing Unnoticed Infinities in Quantum Mechanics Implies a Physical Mechanism for Wave Function Collapse

by g2author, MSEE

## What the author knows and doesn't
I'm a retired electronics engineer and also a lifetime physics addict, so I'm fairly well-versed in what I think of as time-domain physics. However, I have not spent years with my head submerged in that horse trough of math which is quantum mechanics, so this little missive is sadly likely to contain one or two divide-by-zero errors that would have been obvious if I knew the wave-domain math.

## Time-domain physics
Electronic engineering routinely switches between time-domain calculations and frequency-domain calculations, and this is a good abstraction for the relationship between the time-domain physics that can perform many useful calculations much more easily than those same calculations can be performed in the wave-domain physics of quantum mechanics and vice-versa. I perhaps should mention that time-domain physics can't answer quantum mechanical questions, but it enfolds the great wealth of knowledge that comes from precalculated wave-domain results transformed into the time domain.

Because the overlap of results from the two kinds of calculations must be exact wherever it is non-null, it holds that anything impossible in one domain must also be impossible in the other domain in any area of non-null overlap. One obvious impossibility in the time domain of physics is that any particle, real or virtual, must not depend on infinite information for its existence based on the assumption that physically extant information has non-zero mass-energy.

In other words, the use of infinite precision value parameters in wave-domain mathematics transforms to a requirement for infinite mass-energy in the time domain. These infinite precision values are the unnoticed infinities that need to be removed. 

## Conserving information in the time domain
In the time-domain view, a particle interaction consists of some non-zero number of particles (no need to rule out self-interaction) entering a state of quantum superposition, and then, for reasons unknown, leaving that state of superposition in a statistically predictable pattern as some new non-zero number of particles with many conserved properties.

There are perhaps multiple valid ways to map the information contained by a particle to the various "actual" physical properties of that particle. One correspondence which seems inescapable is that the mass-energy of a particle's information must somehow map to a finite fraction of the particle's entire mass-energy. Parsimonious design suggests that this fraction is very close to one for particles of even modest mass, with the bulk of the information contained in potentially multi-dimensional value magnitudes plus perhaps some small amount of formatting information to define the property's behavior.

But having drawn the correspondence between information and mass-energy, we find that the time domain view of physics is telling us that the total information about the particles' states before and after their interactions must be conserved. This implies that the that wave-domain view of information conservation, with its need for potentially unlimited resolution in the math of the wave function, must be only approximately correct.

In other words, the time-domain view of conservation of information is that the finite mass-energy of particles demands that accurate modeling of particle interactions be done with parameters of finite precision.

## Finite precision math
Of course quantizing presents itself immediately as a way of limiting precision, but that is not the only possibility. The mathematical limitation is not that the values be quantized, but that they be compressible to finite size. I don't offhand see how this might work, but I would not put it past the universe to have compressed representations of square and cube roots which are finite but exact.

## Lossy versus lossless math
I presume it's trivially obvious that in the universe of calculations which can be done with limited precision there will be paths of calculation that are informationally lossy and others which are informationally losslesss. An example of a lossless interaction would be a photon reflecting normally.

The vast majority of lossless interactions will be much more complex, of course. It's trivially obvious that whatever kind of calculations happen amongst the quarks of a helium nucleus, those calculation paths are lossless. The same applies to the myriad atomic-level interactions that happen within solid matter.

But that inter-atomic losslessness breaks down with state changes into higher temperatures. Liquids and gasses lack the physical structure to constrain their calculation paths to those which are lossless. In fact, the transition from lossless to lossy in some part of the calculation *is* the phase change. It is computational loss of precision that breaks the structure. The particles have forgotten that they used to be orderly in a way that kept their interactions lossless.

This same kind of stability offers an explanation for radioactivity and isolated neutron decay: stable collections are those whose computational paths tend to put them back into numerically stable computational paths when perturbed. If an object like an unstable isotope decays, it is because its internal interactions wandered into an unlikely but possible numerical sequence of calculations which lose enough precision that their next states do not return to the original stable path set.

It is not central to the lossy/lossless argument, but if we again appeal to parsimony, it seems almost unavoidable that these stable computational paths will map directly to dynamic 3D physical topologies that represent the "formatting" information, thus leaving all the mass-energy available to represent value precision.

## Collapsing the wave function
Finite precision numerical stability also gives an explanation for wave function collapse: superposition is maintained as long as the aggregate information of the system is capable of maintaining the superposition. If its information level drops below the information of the full superposition, the calculational path will do something specific based on where it is when the loss of precision causes the first unrecoverable error.

## Defining Causality
My reasoning about finite precision math is the main thing I wanted to have critiqued, so I could, and perhaps should, stop here.

But by pursuing the not-unreasonable abstraction of correlating information one-to-one with mass-energy, it becomes obvious that mass-energy is a generic way of referring to a particle's ability to affect the results of future calculations. From here the term *causality* starts to look irresistable as a name for that abstraction, and we can trivially observe that causality is conserved, since it is simply another face of mass-energy.

This, finally, tells us that in the absence of structure, the mathematical past is finite&mdash;on a timescale of seconds for hot dense gas. The notion of finite past demands that the past be divided into causal, where the parameters of those past states are still affecting the present, and non-causal, where no matter what those past states were they no longer have any effect on the present. Future is similarly causal and non-causal.

## Parting Shot
Finally, finite precision storage of parameters provides a nice mechanical explanation for linked properties such as position-momentum: they use the same storage space, and are thus fundamentally two aspects of one parameter. This logic continues nicely with mass-energy, which then leads directly to a single storage value that contains the particle's entire causality. Computationally, the bigger that number is relative to other particles, the farther it can project its effects into the future before lossy interactions steal all of its causality.

So if this last little speculation is not complete bullshit, then stable particles must be defined by a set of stable self-interactions using finite precision math, and it becomes impossible to avoid the concept of a quantized unit of mass-energy which is somehow a unit of causality&mdash;perhaps just a single mathematical stored bit.

In the extremely unlikely event that any of this holds up, I nominate the term "fleek", as in "on fleek", for this quantum of causality, I and will explain why if that is ever requested.

I want to offer my sincere thanks for the time you have spent reading this, and I look forward to hearing why I'm completely wrong. :-)
